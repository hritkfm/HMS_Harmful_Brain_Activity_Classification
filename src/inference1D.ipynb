{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import gc\n",
    "import sys\n",
    "import copy\n",
    "import pywt\n",
    "import argparse\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from typing import List, Dict\n",
    "from dataclasses import dataclass, field\n",
    "from pathlib import Path\n",
    "from tqdm.notebook import tqdm\n",
    "from omegaconf import OmegaConf\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================\n",
      "mode : val\n",
      "env  : LOCAL\n",
      "debug: False\n",
      "========================\n"
     ]
    }
   ],
   "source": [
    "MODE = \"val\" # \"test\" or \"val\"\n",
    "OUTPUT_FOR_ENSEMBLE = True\n",
    "DEBUG = False #False #True\n",
    "DEBUG_SAMPLE_NUM = 100\n",
    "KERNEL = Path(\"/kaggle/input/hms-harmful-brain-activity-classification\").exists()\n",
    "\n",
    "CLASSES = [\n",
    "    \"seizure_vote\",\n",
    "    \"lpd_vote\",\n",
    "    \"gpd_vote\",\n",
    "    \"lrda_vote\",\n",
    "    \"grda_vote\",\n",
    "    \"other_vote\",\n",
    "]\n",
    "DATA = (\n",
    "    Path(\"/kaggle/input/hms-harmful-brain-activity-classification\")\n",
    "    if KERNEL\n",
    "    else Path(\"../hms-harmful-brain-activity-classification\")\n",
    ")\n",
    "OUTPUT = Path(\"./\") if KERNEL else Path(\"../submissions\")\n",
    "if not KERNEL:\n",
    "    OUTPUT.mkdir(parents=True, exist_ok=True)\n",
    "TMP = Path(\"./.tmp\") if KERNEL else None\n",
    "CKPTS = Path(\"\") if KERNEL else Path(\"../checkpoints/\")\n",
    "\n",
    "\n",
    "print(\"========================\")\n",
    "print(\"mode :\", MODE)\n",
    "print(\"env  :\", \"KERNEL\" if KERNEL else \"LOCAL\")\n",
    "print(\"debug:\", DEBUG)\n",
    "print(\"========================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-06 20:38:13.935121: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /opt/conda/lib/python3.10/site-packages/cv2/../../lib64:/usr/local/cuda/lib64:/usr/local/cuda/lib:/usr/local/lib/x86_64-linux-gnu:/usr/local/nvidia/lib:/usr/local/nvidia/lib64:/usr/local/nvidia/lib:/usr/local/nvidia/lib64:/opt/conda/lib\n",
      "2024-04-06 20:38:13.935278: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /opt/conda/lib/python3.10/site-packages/cv2/../../lib64:/usr/local/cuda/lib64:/usr/local/cuda/lib:/usr/local/lib/x86_64-linux-gnu:/usr/local/nvidia/lib:/usr/local/nvidia/lib64:/usr/local/nvidia/lib:/usr/local/nvidia/lib64:/opt/conda/lib\n",
      "2024-04-06 20:38:13.935293: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "if KERNEL:\n",
    "    sys.path.append(os.path.join(os.getcwd(), \"/kaggle/input/hms-hbac-src\"))\n",
    "else:\n",
    "    sys.path.append(os.path.join(os.getcwd(), \"../src\"))\n",
    "from mydatasets import HMSHBACSpecDataset, HMSSEDDataset, HMS1DDataset\n",
    "from mydatasets.make_eeg_spectrograms import spectrogram_from_eeg\n",
    "from augmentations import hms_spec_augmentations, hms_1D_augmentations\n",
    "from metrics.kaggle_kl_div import score as calc_kl_div\n",
    "import models as MODELS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "if MODE == \"val\":\n",
    "    csv_path = DATA / \"train.csv\" if KERNEL else DATA / \"train_fold_irr_mark_v2.csv\" # for foldv1\n",
    "    # csv_path = DATA / \"train.csv\" if KERNEL else DATA / \"train_fold_irr_mark_v5.csv\" # for foldv2\n",
    "    # csv_path = DATA / \"train.csv\"\n",
    "    spec_dir_path = DATA / \"train_spectrograms\"\n",
    "    eeg_dir_path = DATA / \"train_eegs\"\n",
    "else:\n",
    "    csv_path = DATA / \"test.csv\"\n",
    "    spec_dir_path = DATA / \"test_spectrograms\"\n",
    "    eeg_dir_path = DATA / \"test_eegs\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class CFG:\n",
    "    gpu: int = 0\n",
    "    batch_size: int = 16\n",
    "    n_workers: int = 1\n",
    "    k_fold: int = 4\n",
    "    checkpoints: List[str] = field(\n",
    "        default_factory=lambda: [\n",
    "            # [\n",
    "            #     \"run-20240211_185534-pu295w5p-KSpec2D_V2_effnetb0_fold_0/epoch=09-val_loss=0.608.ckpt\",\n",
    "            #     \"run-20240211_192558-ovc53rmi-KSpec2D_V2_effnetb0_fold_1/epoch=09-val_loss=0.600.ckpt\",\n",
    "            #     \"run-20240211_195609-ie2mxejz-KSpec2D_V2_effnetb0_fold_2/epoch=09-val_loss=0.659.ckpt\",\n",
    "            #     \"run-20240211_202650-289cskok-KSpec2D_V2_effnetb0_fold_3/epoch=08-val_loss=0.640.ckpt\",\n",
    "            # ],\n",
    "            # [\n",
    "            #     \"run-20240211_185541-xw7lf0ob-ESpec2D_V2_effnetb0_fold_0/epoch=09-val_loss=0.610.ckpt\",\n",
    "            #     \"run-20240211_192553-rdgt2uon-ESpec2D_V2_effnetb0_fold_1/epoch=09-val_loss=0.613.ckpt\",\n",
    "            #     \"run-20240211_195622-djkb3hxq-ESpec2D_V2_effnetb0_fold_2/epoch=09-val_loss=0.655.ckpt\",\n",
    "            #     \"run-20240211_202629-kvyxif7d-ESpec2D_V2_effnetb0_fold_3/epoch=09-val_loss=0.618.ckpt\",\n",
    "            # ],\n",
    "            # LB=0.26: base モデル\n",
    "            # [\n",
    "            #     \"run-20240312_102706-0an6kdp8-1D_cls_RTpIcS_LS_Wavenet-maxxvitv2n_1e-3_standard_e50_warmup_finetune1e-4-nv10_e30_fold_0/epoch=21-val_metric_kldiv_high_votes=0.256.ckpt\",\n",
    "            #     \"run-20240312_134457-fq6753ul-1D_cls_RTpIcS_LS_Wavenet-maxxvitv2n_1e-3_standard_e50_warmup_finetune1e-4-nv10_e30_fold_1/epoch=08-val_metric_kldiv_high_votes=0.243.ckpt\",\n",
    "            #     \"run-20240312_134503-uk71e5ex-1D_cls_RTpIcS_LS_Wavenet-maxxvitv2n_1e-3_standard_e50_warmup_finetune1e-4-nv10_e30_fold_2/epoch=18-val_metric_kldiv_high_votes=0.239.ckpt\",\n",
    "            #     \"run-20240312_155203-xcob14sf-1D_cls_RTpIcS_LS_Wavenet-maxxvitv2n_1e-3_standard_e50_warmup_finetune1e-4-nv10_e30_fold_3/epoch=23-val_metric_kldiv_high_votes=0.249.ckpt\",\n",
    "            # ],\n",
    "            # LB=0.25: downsampleモデル\n",
    "            # [\n",
    "            #     \"run-20240315_153328-zrrn4qhx-1D_cls_RTpIcS_LS_Wavenet-maxxvitv2n_1e-3_standard_e50_warmup_downwample_finetune_fold_0/epoch=16-val_metric_kldiv_high_votes=0.249.ckpt\",\n",
    "            #     \"run-20240315_153438-rqwbpbfb-1D_cls_RTpIcS_LS_Wavenet-maxxvitv2n_1e-3_standard_e50_warmup_downwample_finetune_fold_1/epoch=25-val_metric_kldiv_high_votes=0.238.ckpt\",\n",
    "            #     \"run-20240315_153442-0m77mgei-1D_cls_RTpIcS_LS_Wavenet-maxxvitv2n_1e-3_standard_e50_warmup_downwample_finetune_fold_2/epoch=28-val_metric_kldiv_high_votes=0.239.ckpt\",\n",
    "            #     \"run-20240315_153445-ujddmobr-1D_cls_RTpIcS_LS_Wavenet-maxxvitv2n_1e-3_standard_e50_warmup_downwample_finetune_fold_3/epoch=22-val_metric_kldiv_high_votes=0.252.ckpt\",\n",
    "            # ],\n",
    "            # [\n",
    "            #     \"run-20240321_060327-lsxf2mo1-1D_cls_RTpIcS_LS_Wavenet-mmaxxvitv2n_1e-3_standard_e50_warmup_downwample_wo_3vote_fold_0/epoch=40-val_metric_kldiv=0.500.ckpt\",\n",
    "            #     \"run-20240321_060509-vujozila-1D_cls_RTpIcS_LS_Wavenet-mmaxxvitv2n_1e-3_standard_e50_warmup_downwample_wo_3vote_fold_1/epoch=42-val_metric_kldiv=0.484.ckpt\",\n",
    "            #     \"run-20240321_060511-w5rlvmqh-1D_cls_RTpIcS_LS_Wavenet-mmaxxvitv2n_1e-3_standard_e50_warmup_downwample_wo_3vote_fold_2/epoch=49-val_metric_kldiv=0.543.ckpt\",\n",
    "            #     \"run-20240321_060512-9a1gq2fd-1D_cls_RTpIcS_LS_Wavenet-mmaxxvitv2n_1e-3_standard_e50_warmup_downwample_wo_3vote_fold_3/epoch=32-val_metric_kldiv=0.501.ckpt\",\n",
    "            # ]\n",
    "            # pseudo label用にlastのモデルを使用(リークしないようにするため)\n",
    "            # [\n",
    "            #     \"run-20240322_065000-u71obv0s-1D_cls_RTpIcS_LS_Wavenet-mmaxxvitv2n_1e-3_standard_e50_warmup_downwample_for_pseudo_fold_0/last.ckpt\",\n",
    "            #     \"run-20240322_065002-bu1tybcq-1D_cls_RTpIcS_LS_Wavenet-mmaxxvitv2n_1e-3_standard_e50_warmup_downwample_for_pseudo_fold_1/last.ckpt\",\n",
    "            #     \"run-20240322_065004-6jrvbpke-1D_cls_RTpIcS_LS_Wavenet-mmaxxvitv2n_1e-3_standard_e50_warmup_downwample_for_pseudo_fold_2/last.ckpt\",\n",
    "            #     \"run-20240322_065005-6rexi3fr-1D_cls_RTpIcS_LS_Wavenet-mmaxxvitv2n_1e-3_standard_e50_warmup_downwample_for_pseudo_fold_3/last.ckpt\",\n",
    "            # ]\n",
    "            # pseudo label(Soft)を使った全データ学習モデル\n",
    "            # [\n",
    "            # \"run-20240322_202204-zfdkl8yy-1D_cls_RTpIcS_LS_Wavenet-mmaxxvitv2n_1e-3_standard_e50_warmup_downwample_soft_pseudo_fold_0/epoch=30-val_metric_kldiv_high_votes=0.293.ckpt\",\n",
    "            # \"run-20240322_202206-wl0ys2fj-1D_cls_RTpIcS_LS_Wavenet-mmaxxvitv2n_1e-3_standard_e50_warmup_downwample_soft_pseudo_fold_1/epoch=22-val_metric_kldiv_high_votes=0.287.ckpt\",\n",
    "            # \"run-20240322_202208-57bowrzt-1D_cls_RTpIcS_LS_Wavenet-mmaxxvitv2n_1e-3_standard_e50_warmup_downwample_soft_pseudo_fold_2/epoch=34-val_metric_kldiv_high_votes=0.291.ckpt\",\n",
    "            # \"run-20240322_202209-ptn3xio3-1D_cls_RTpIcS_LS_Wavenet-mmaxxvitv2n_1e-3_standard_e50_warmup_downwample_soft_pseudo_fold_3/epoch=36-val_metric_kldiv_high_votes=0.279.ckpt\",\n",
    "            # ]\n",
    "            # downsampleモデルのSeed=0, CV=0.252\n",
    "            # [\n",
    "            #     \"run-20240401_111123-vuot3hc1-1D_cls_RTpIcS_LS_Wavenet-maxxvitv2n_1e-3_standard_e50_warmup_downwample_seed0_finetune_fold_0/epoch=10-val_metric_kldiv_high_votes=0.259.ckpt\",\n",
    "            #     \"run-20240401_124844-1abnrom1-1D_cls_RTpIcS_LS_Wavenet-maxxvitv2n_1e-3_standard_e50_warmup_downwample_seed0_finetune_fold_1/epoch=18-val_metric_kldiv_high_votes=0.250.ckpt\",\n",
    "            #     \"run-20240401_142450-1ouy8vnm-1D_cls_RTpIcS_LS_Wavenet-maxxvitv2n_1e-3_standard_e50_warmup_downwample_seed0_finetune_fold_2/epoch=27-val_metric_kldiv_high_votes=0.248.ckpt\",\n",
    "            #     \"run-20240401_160306-ierghoep-1D_cls_RTpIcS_LS_Wavenet-maxxvitv2n_1e-3_standard_e50_warmup_downwample_seed0_finetune_fold_3/epoch=20-val_metric_kldiv_high_votes=0.252.ckpt\",\n",
    "            # ],\n",
    "            # downsampleモデルのSeed=123, CV=0.247\n",
    "            # [\n",
    "            #     \"run-20240401_182516-c3nkrdyq-1D_cls_RTpIcS_LS_Wavenet-maxxvitv2n_1e-3_standard_e50_warmup_downwample_seed123_finetune_fold_0/epoch=24-val_metric_kldiv_high_votes=0.258.ckpt\",\n",
    "            #     \"run-20240401_123308-ycz11bsh-1D_cls_RTpIcS_LS_Wavenet-maxxvitv2n_1e-3_standard_e50_warmup_downwample_seed123_finetune_fold_1/epoch=26-val_metric_kldiv_high_votes=0.240.ckpt\",\n",
    "            #     \"run-20240401_140851-akcr7s7k-1D_cls_RTpIcS_LS_Wavenet-maxxvitv2n_1e-3_standard_e50_warmup_downwample_seed123_finetune_fold_2/epoch=19-val_metric_kldiv_high_votes=0.240.ckpt\",\n",
    "            #     \"run-20240401_154720-i5m93pz4-1D_cls_RTpIcS_LS_Wavenet-maxxvitv2n_1e-3_standard_e50_warmup_downwample_seed123_finetune_fold_3/epoch=28-val_metric_kldiv_high_votes=0.251.ckpt\",\n",
    "            # ],\n",
    "            # downsampleモデルのbackbone = maxxvit_rmlp_small, CV=0.245\n",
    "            # [\n",
    "            #     \"run-20240401_111126-xo5scl7d-1D_cls_RTpIcS_LS_Wavenet-maxxvits_1e-3_standard_e50_warmup_downwample_finetune_fold_0/epoch=16-val_metric_kldiv_high_votes=0.256.ckpt\",\n",
    "            #     \"run-20240401_125917-uay4ug48-1D_cls_RTpIcS_LS_Wavenet-maxxvits_1e-3_standard_e50_warmup_downwample_finetune_fold_1/epoch=29-val_metric_kldiv_high_votes=0.232.ckpt\",\n",
    "            #     \"run-20240401_144726-czyaecih-1D_cls_RTpIcS_LS_Wavenet-maxxvits_1e-3_standard_e50_warmup_downwample_finetune_fold_2/epoch=21-val_metric_kldiv_high_votes=0.246.ckpt\",\n",
    "            #     \"run-20240401_163800-65gz7sk5-1D_cls_RTpIcS_LS_Wavenet-maxxvits_1e-3_standard_e50_warmup_downwample_finetune_fold_3/epoch=08-val_metric_kldiv_high_votes=0.247.ckpt\",\n",
    "            # ],\n",
    "            # downsampleモデルのbackbone = tf_efficientnet_b4_ns, CV=0.269\n",
    "            # [\n",
    "            #     \"run-20240401_111124-9y3ixw3i-1D_cls_RTpIcS_LS_Wavenet-effnetb4_1e-3_standard_e50_warmup_downwample_finetune_fold_0/epoch=11-val_metric_kldiv_high_votes=0.275.ckpt\",\n",
    "            #     \"run-20240401_124721-6uxqz0en-1D_cls_RTpIcS_LS_Wavenet-effnetb4_1e-3_standard_e50_warmup_downwample_finetune_fold_1/epoch=07-val_metric_kldiv_high_votes=0.269.ckpt\",\n",
    "            #     \"run-20240401_142250-lsohp5jh-1D_cls_RTpIcS_LS_Wavenet-effnetb4_1e-3_standard_e50_warmup_downwample_finetune_fold_2/epoch=06-val_metric_kldiv_high_votes=0.264.ckpt\",\n",
    "            #     \"run-20240401_160029-fdugba01-1D_cls_RTpIcS_LS_Wavenet-effnetb4_1e-3_standard_e50_warmup_downwample_finetune_fold_3/epoch=07-val_metric_kldiv_high_votes=0.268.ckpt\",\n",
    "            # ],\n",
    "            # downsampleモデルのbackbone = coatnet_0_rw_224, CV=0.271\n",
    "            # [\n",
    "            #     \"run-20240403_173155-e8o9lftc-1D_cls_RTpIcS_LS_Wavenet-coatnet0_1e-3_standard_e50_warmup_downwample_finetune_fold_0/epoch=28-val_metric_kldiv_high_votes=0.283.ckpt\",\n",
    "            #     \"run-20240403_173156-83p7voi3-1D_cls_RTpIcS_LS_Wavenet-coatnet0_1e-3_standard_e50_warmup_downwample_finetune_fold_1/epoch=19-val_metric_kldiv_high_votes=0.270.ckpt\",\n",
    "            #     \"run-20240403_173157-7vqt1svv-1D_cls_RTpIcS_LS_Wavenet-coatnet0_1e-3_standard_e50_warmup_downwample_finetune_fold_2/epoch=11-val_metric_kldiv_high_votes=0.266.ckpt\",\n",
    "            #     \"run-20240403_173159-tz8637ay-1D_cls_RTpIcS_LS_Wavenet-coatnet0_1e-3_standard_e50_warmup_downwample_finetune_fold_3/epoch=14-val_metric_kldiv_high_votes=0.273.ckpt\",\n",
    "            # ],\n",
    "\n",
    "            # downsampleモデルのbackbone = maxxvitv2_nano_rw_256, fold変え CV=0.239\n",
    "            [\n",
    "                \"run-20240405_152214-wm5w0ew3-1D_cls_RTpIcS_LS_Wavenet-maxxvitv2n_1e-3_standard_e50_warmup_downwample_foldV2_finetune_fold_0/epoch=28-val_metric_kldiv_high_votes=0.240.ckpt\",\n",
    "                \"run-20240405_152227-th1rruwk-1D_cls_RTpIcS_LS_Wavenet-maxxvitv2n_1e-3_standard_e50_warmup_downwample_foldV2_finetune_fold_1/epoch=20-val_metric_kldiv_high_votes=0.218.ckpt\",\n",
    "                \"run-20240405_152228-xhwtkxba-1D_cls_RTpIcS_LS_Wavenet-maxxvitv2n_1e-3_standard_e50_warmup_downwample_foldV2_finetune_fold_2/epoch=25-val_metric_kldiv_high_votes=0.257.ckpt\",\n",
    "                \"run-20240405_165938-x123rriq-1D_cls_RTpIcS_LS_Wavenet-maxxvitv2n_1e-3_standard_e50_warmup_downwample_foldV2_finetune_fold_3/epoch=21-val_metric_kldiv_high_votes=0.241.ckpt\",\n",
    "            ],\n",
    "            # downsampleモデルのbackbone = maxxvit_rmlp_small_rw_256, fold変え CV=0.240\n",
    "            [\n",
    "                \"run-20240405_170053-kadak6et-1D_cls_RTpIcS_LS_Wavenet-maxxvits_1e-3_standard_e50_warmup_downwample_foldV2_finetune_fold_0/epoch=27-val_metric_kldiv_high_votes=0.240.ckpt\",\n",
    "                \"run-20240405_165949-lbjfygkz-1D_cls_RTpIcS_LS_Wavenet-maxxvits_1e-3_standard_e50_warmup_downwample_foldV2_finetune_fold_1/epoch=27-val_metric_kldiv_high_votes=0.216.ckpt\",\n",
    "                \"run-20240405_183700-wgsxjdxt-1D_cls_RTpIcS_LS_Wavenet-maxxvits_1e-3_standard_e50_warmup_downwample_foldV2_finetune_fold_2/epoch=27-val_metric_kldiv_high_votes=0.255.ckpt\",\n",
    "                \"run-20240405_184943-9u133n8x-1D_cls_RTpIcS_LS_Wavenet-maxxvits_1e-3_standard_e50_warmup_downwample_foldV2_finetune_fold_3/epoch=06-val_metric_kldiv_high_votes=0.250.ckpt\",\n",
    "            ],\n",
    "            # downsampleモデルのbackbone = tf_efficientnet_b4_ns, fold変え CV=0.265\n",
    "            [\n",
    "                \"run-20240405_185147-8gjfntzq-1D_cls_RTpIcS_LS_Wavenet-effnetb4_1e-3_standard_e15_warmup_downwample_foldV2_finetune_fold_0/epoch=06-val_metric_kldiv_high_votes=0.272.ckpt\",\n",
    "                \"run-20240405_202552-4q5x7wi1-1D_cls_RTpIcS_LS_Wavenet-effnetb4_1e-3_standard_e15_warmup_downwample_foldV2_finetune_fold_1/epoch=08-val_metric_kldiv_high_votes=0.240.ckpt\",\n",
    "                \"run-20240405_203742-j6v7ia0d-1D_cls_RTpIcS_LS_Wavenet-effnetb4_1e-3_standard_e15_warmup_downwample_foldV2_finetune_fold_2/epoch=09-val_metric_kldiv_high_votes=0.276.ckpt\",\n",
    "                \"run-20240405_195044-wteo0zct-1D_cls_RTpIcS_LS_Wavenet-effnetb4_1e-3_standard_e15_warmup_downwample_foldV2_finetune_fold_3/epoch=04-val_metric_kldiv_high_votes=0.273.ckpt\",\n",
    "            ],\n",
    "\n",
    "            ### 1st Stage Model (all data training)\n",
    "            # # wavenet_maxxvitv2n_alldata\n",
    "            # [\n",
    "            #     \"run-20240311_162443-wtgvtt7f-1D_cls_RTpIcS_LS_Wavenet-maxxvitv2n_1e-3_standard_e50_warmup_fold_0/epoch=48-val_metric_kldiv=0.506.ckpt\",\n",
    "            #     \"run-20240311_162558-52u6k3pz-1D_cls_RTpIcS_LS_Wavenet-maxxvitv2n_1e-3_standard_e50_warmup_fold_1/epoch=39-val_metric_kldiv=0.483.ckpt\",\n",
    "            #     \"run-20240311_162601-19rayuro-1D_cls_RTpIcS_LS_Wavenet-maxxvitv2n_1e-3_standard_e50_warmup_fold_2/epoch=43-val_metric_kldiv=0.531.ckpt\",\n",
    "            #     \"run-20240311_162604-vhwn8ok2-1D_cls_RTpIcS_LS_Wavenet-maxxvitv2n_1e-3_standard_e50_warmup_fold_3/epoch=43-val_metric_kldiv=0.501.ckpt\",\n",
    "            # ],\n",
    "            # # wavenet_maxxvitv2n_downsample_alldata\n",
    "            # [\n",
    "            #     \"run-20240315_093258-4vcdh3kr-1D_cls_RTpIcS_LS_Wavenet-maxxvitv2n_1e-3_standard_e50_warmup_downwample_fold_0/epoch=48-val_metric_kldiv=0.494.ckpt\",\n",
    "            #     \"run-20240315_093300-e1zliaye-1D_cls_RTpIcS_LS_Wavenet-maxxvitv2n_1e-3_standard_e50_warmup_downwample_fold_1/epoch=43-val_metric_kldiv=0.496.ckpt\",\n",
    "            #     \"run-20240315_093302-eh3bfg7h-1D_cls_RTpIcS_LS_Wavenet-maxxvitv2n_1e-3_standard_e50_warmup_downwample_fold_2/epoch=40-val_metric_kldiv=0.541.ckpt\",\n",
    "            #     \"run-20240315_093304-3206t2wv-1D_cls_RTpIcS_LS_Wavenet-maxxvitv2n_1e-3_standard_e50_warmup_downwample_fold_3/epoch=35-val_metric_kldiv=0.505.ckpt\",\n",
    "            # ],\n",
    "            # # wavenet_maxxvitv2n_downsample_seed0_alldata\n",
    "            # [\n",
    "            #     \"run-20240331_092207-wd9z8fgc-1D_cls_RTpIcS_LS_Wavenet-maxxvitv2n_1e-3_standard_e50_warmup_downwample_seed0_fold_0/epoch=47-val_metric_kldiv=0.498.ckpt\",\n",
    "            #     \"run-20240331_145603-qwo5bx04-1D_cls_RTpIcS_LS_Wavenet-maxxvitv2n_1e-3_standard_e50_warmup_downwample_seed0_fold_1/epoch=39-val_metric_kldiv=0.496.ckpt\",\n",
    "            #     \"run-20240331_202910-hmm3yowr-1D_cls_RTpIcS_LS_Wavenet-maxxvitv2n_1e-3_standard_e50_warmup_downwample_seed0_fold_2/epoch=31-val_metric_kldiv=0.583.ckpt\",\n",
    "            #     \"run-20240401_020514-w85fm98b-1D_cls_RTpIcS_LS_Wavenet-maxxvitv2n_1e-3_standard_e50_warmup_downwample_seed0_fold_3/epoch=45-val_metric_kldiv=0.496.ckpt\",\n",
    "            # ],\n",
    "            # # wavenet_maxxvitv2n_downsample_seed123_alldata\n",
    "            # [\n",
    "            #     \"run-20240331_092204-83irzg77-1D_cls_RTpIcS_LS_Wavenet-maxxvitv2n_1e-3_standard_e50_warmup_downwample_seed123_fold_0/epoch=48-val_metric_kldiv=0.499.ckpt\",\n",
    "            #     \"run-20240331_145639-riq6hh5i-1D_cls_RTpIcS_LS_Wavenet-maxxvitv2n_1e-3_standard_e50_warmup_downwample_seed123_fold_1/epoch=45-val_metric_kldiv=0.497.ckpt\",\n",
    "            #     \"run-20240331_203051-j6nfot42-1D_cls_RTpIcS_LS_Wavenet-maxxvitv2n_1e-3_standard_e50_warmup_downwample_seed123_fold_2/epoch=40-val_metric_kldiv=0.546.ckpt\",\n",
    "            #     \"run-20240401_020520-vra67vnh-1D_cls_RTpIcS_LS_Wavenet-maxxvitv2n_1e-3_standard_e50_warmup_downwample_seed123_fold_3/epoch=46-val_metric_kldiv=0.497.ckpt\",\n",
    "            # ],\n",
    "            # # wavenet_maxxvits_downsample_alldata\n",
    "            # [\n",
    "            #     \"run-20240331_092730-shfd8ff5-1D_cls_RTpIcS_LS_Wavenet-maxxvits_1e-3_standard_e50_warmup_downwample_fold_0/epoch=29-val_metric_kldiv=0.505.ckpt\",\n",
    "            #     \"run-20240331_155222-zgqkmvju-1D_cls_RTpIcS_LS_Wavenet-maxxvits_1e-3_standard_e50_warmup_downwample_fold_1/epoch=14-val_metric_kldiv=0.493.ckpt\",\n",
    "            #     \"run-20240331_221452-l8lmiy5f-1D_cls_RTpIcS_LS_Wavenet-maxxvits_1e-3_standard_e50_warmup_downwample_fold_2/epoch=30-val_metric_kldiv=0.571.ckpt\",\n",
    "            #     \"run-20240401_044218-99wn003l-1D_cls_RTpIcS_LS_Wavenet-maxxvits_1e-3_standard_e50_warmup_downwample_fold_3/epoch=26-val_metric_kldiv=0.513.ckpt\",\n",
    "            # ],\n",
    "            # # wavenet_effnetb4_downsample_alldata\n",
    "            # [\n",
    "            #     \"run-20240331_093954-xmakusup-1D_cls_RTpIcS_LS_Wavenet-effnetb4_1e-3_standard_e50_warmup_downwample_fold_0/epoch=19-val_metric_kldiv=0.534.ckpt\",\n",
    "            #     \"run-20240331_151353-u3jqgek0-1D_cls_RTpIcS_LS_Wavenet-effnetb4_1e-3_standard_e50_warmup_downwample_fold_1/epoch=20-val_metric_kldiv=0.543.ckpt\",\n",
    "            #     \"run-20240331_204609-t825jlg5-1D_cls_RTpIcS_LS_Wavenet-effnetb4_1e-3_standard_e50_warmup_downwample_fold_2/epoch=17-val_metric_kldiv=0.589.ckpt\",\n",
    "            #     \"run-20240401_022450-mpyf84lx-1D_cls_RTpIcS_LS_Wavenet-effnetb4_1e-3_standard_e50_warmup_downwample_fold_3/epoch=14-val_metric_kldiv=0.551.ckpt\",\n",
    "            # ],\n",
    "            # # wavenet_maxxvitv2n_downsample_foldV2_alldata\n",
    "            # [\n",
    "            #     \"run-20240404_093006-hcrzbbd6-1D_cls_RTpIcS_LS_Wavenet-maxxvitv2n_1e-3_standard_e50_warmup_downwample_foldV2_fold_0/epoch=34-val_metric_kldiv=0.489.ckpt\",\n",
    "            #     \"run-20240404_145752-5ltl9fmz-1D_cls_RTpIcS_LS_Wavenet-maxxvitv2n_1e-3_standard_e50_warmup_downwample_foldV2_fold_1/epoch=46-val_metric_kldiv=0.516.ckpt\",\n",
    "            #     \"run-20240404_204001-ca7kjo4a-1D_cls_RTpIcS_LS_Wavenet-maxxvitv2n_1e-3_standard_e50_warmup_downwample_foldV2_fold_2/epoch=39-val_metric_kldiv=0.491.ckpt\",\n",
    "            #     \"run-20240405_020131-hledwno7-1D_cls_RTpIcS_LS_Wavenet-maxxvitv2n_1e-3_standard_e50_warmup_downwample_foldV2_fold_3/epoch=49-val_metric_kldiv=0.535.ckpt\",\n",
    "            # ],\n",
    "            # # wavenet_maxxvits_downsample_foldV2_alldata\n",
    "            # [\n",
    "            #     \"run-20240404_093006-xk4w5c6h-1D_cls_RTpIcS_LS_Wavenet-maxxvits_1e-3_standard_e50_warmup_downwample_foldV2_fold_0/epoch=21-val_metric_kldiv=0.509.ckpt\",\n",
    "            #     \"run-20240404_155631-2bzb9zia-1D_cls_RTpIcS_LS_Wavenet-maxxvits_1e-3_standard_e50_warmup_downwample_foldV2_fold_1/epoch=24-val_metric_kldiv=0.514.ckpt\",\n",
    "            #     \"run-20240404_222049-avllzbz1-1D_cls_RTpIcS_LS_Wavenet-maxxvits_1e-3_standard_e50_warmup_downwample_foldV2_fold_2/epoch=25-val_metric_kldiv=0.505.ckpt\",\n",
    "            #     \"run-20240405_042525-hi4n1g20-1D_cls_RTpIcS_LS_Wavenet-maxxvits_1e-3_standard_e50_warmup_downwample_foldV2_fold_3/epoch=34-val_metric_kldiv=0.549.ckpt\",\n",
    "            # ],\n",
    "            # # wavenet_effnetb4_downsample_foldV2_alldata\n",
    "            # [\n",
    "            #     \"run-20240404_150430-6cspsrdh-1D_cls_RTpIcS_LS_Wavenet-effnetb4_1e-3_standard_e15_warmup_downwample_foldV2_fold_0/epoch=10-val_metric_kldiv=0.532.ckpt\",\n",
    "            #     \"run-20240404_170157-lf2uzpt4-1D_cls_RTpIcS_LS_Wavenet-effnetb4_1e-3_standard_e15_warmup_downwample_foldV2_fold_1/epoch=14-val_metric_kldiv=0.541.ckpt\",\n",
    "            #     \"run-20240404_190202-04eht0lr-1D_cls_RTpIcS_LS_Wavenet-effnetb4_1e-3_standard_e15_warmup_downwample_foldV2_fold_2/epoch=13-val_metric_kldiv=0.504.ckpt\",\n",
    "            #     \"run-20240404_205700-4yn4p2dk-1D_cls_RTpIcS_LS_Wavenet-effnetb4_1e-3_standard_e15_warmup_downwample_foldV2_fold_3/epoch=14-val_metric_kldiv=0.533.ckpt\",\n",
    "            # ],\n",
    "        ]\n",
    "    )\n",
    "    # checkpointsのリストごとにデータセットをどれ使うか指定する。\n",
    "    datasets: List = field(\n",
    "        default_factory=lambda: [\n",
    "            # HMS1DDataset,\n",
    "            # HMS1DDataset,\n",
    "            # HMS1DDataset,\n",
    "            # HMS1DDataset,\n",
    "            # HMS1DDataset,\n",
    "            # HMS1DDataset,\n",
    "            # HMS1DDataset,\n",
    "            HMS1DDataset,\n",
    "            HMS1DDataset,\n",
    "            HMS1DDataset,\n",
    "        ]\n",
    "    ) \n",
    "    # checkpointsのリストごとのweightを決定\n",
    "    # model_weight: List[int] = field(default_factory=lambda: [1, 1, 1, 1, 1, 1])\n",
    "    # model_weight: List[int] = field(default_factory=lambda: [1])\n",
    "    model_weight: List[int] = field(default_factory=lambda: [1, 1, 1])\n",
    "    # Ensembleようにlogit等を保存する際のモデル名\n",
    "    model_name: List[str] = field(\n",
    "        default_factory=lambda: [\n",
    "            ### 2nd stage model\n",
    "            # \"wavenet_maxxvitv2n\",\n",
    "            # \"wavenet_maxxvitv2n_downsample\",\n",
    "            # \"wavenet_maxxvitv2n_downsample_seed0\",\n",
    "            # \"wavenet_maxxvitv2n_downsample_seed123\",\n",
    "            # \"wavenet_maxxvits_downsample\",\n",
    "            # \"wavenet_effnetb4_downsample\",\n",
    "            # \"wavenet_coatnet0_downsample\"\n",
    "            \"wavenet_maxxvitv2n_downsample_foldV2\",\n",
    "            \"wavenet_maxxvits_downsample_foldV2\",\n",
    "            \"wavenet_effnetb4_downsample_foldV2\",\n",
    "\n",
    "            ### 1st stage model (all data)\n",
    "            # \"wavenet_maxxvitv2n_alldata\",\n",
    "            # \"wavenet_maxxvitv2n_downsample_alldata\",\n",
    "            # \"wavenet_maxxvitv2n_downsample_seed0_alldata\",\n",
    "            # \"wavenet_maxxvitv2n_downsample_seed123_alldata\",\n",
    "            # \"wavenet_maxxvits_downsample_alldata\",\n",
    "            # \"wavenet_effnetb4_downsample_alldata\",\n",
    "            # \"wavenet_maxxvitv2n_downsample_foldV2_alldata\",\n",
    "            # \"wavenet_maxxvits_downsample_foldV2_alldata\",\n",
    "            # \"wavenet_effnetb4_downsample_foldV2_alldata\",\n",
    "            \n",
    "        ]\n",
    "    )\n",
    "    # ttaをどれ使うか\n",
    "    tta_type: List[str] = field(default_factory=lambda: [])\n",
    "    # tta_type: List[str] = field(default_factory=lambda: [\"Inversion\"])\n",
    "    # tta_type: List[str] = field(default_factory=lambda: [\"Reverse\"])\n",
    "    # tta_type: List[str] = field(default_factory=lambda: [\"ChannelSwap\"])\n",
    "    # tta_type: List[str] = field(default_factory=lambda: [\"Inversion\", \"Reverse\", \"ChannelSwap\"])\n",
    "\n",
    "    csv_path: Path = csv_path\n",
    "    spec_dir_path: Path = spec_dir_path\n",
    "    eeg_dir_path: Path = eeg_dir_path\n",
    "\n",
    "    # postprocess\n",
    "    scale_coeff: float = 0\n",
    "\n",
    "\n",
    "GLOBAL_CFG = CFG()\n",
    "\n",
    "assert len(GLOBAL_CFG.checkpoints) == len(GLOBAL_CFG.datasets) == len(GLOBAL_CFG.model_name) == len(GLOBAL_CFG.model_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def allkeys(x):\n",
    "    for key, value in x.items():\n",
    "        yield key\n",
    "        if isinstance(value, dict):\n",
    "            for child in allkeys(value):\n",
    "                yield key + \".\" + child\n",
    "\n",
    "\n",
    "def check_dotlist(cfg, dotlist):\n",
    "    cfg_dict = OmegaConf.to_container(cfg, resolve=True)\n",
    "    cfg_keys = list(allkeys(cfg_dict))\n",
    "    dotlist_dict = OmegaConf.to_container(dotlist, resolve=True)\n",
    "    dotlist_keys = list(allkeys(dotlist_dict))\n",
    "\n",
    "    for d_key in dotlist_keys:\n",
    "        assert d_key in cfg_keys, f\"{d_key} dosen't exist in config file.\"\n",
    "\n",
    "\n",
    "def load_configs(checkpoints):\n",
    "    configs_list = []\n",
    "    checkpoints_list = []\n",
    "    for ckpts in checkpoints:\n",
    "        configs = []\n",
    "        ckpt_fold = []\n",
    "        for c in ckpts:\n",
    "            c = CKPTS / c\n",
    "            conf_path = c.parent / \"train_config.yaml\"\n",
    "            conf = OmegaConf.load(conf_path)\n",
    "            ckpt_fold.append(c)\n",
    "            configs.append(conf)\n",
    "        configs_list.append(configs)\n",
    "        checkpoints_list.append(ckpt_fold)\n",
    "\n",
    "    return checkpoints_list, configs_list\n",
    "\n",
    "\n",
    "if DEBUG:\n",
    "    ckpts, configs = load_configs(GLOBAL_CFG.checkpoints)\n",
    "    for i in range(len(ckpts)):\n",
    "        print(ckpts[i][0])\n",
    "        print(configs[i][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_transforms(config):\n",
    "    # transform\n",
    "    height = config.height\n",
    "    width = config.width\n",
    "    augment_args = config.transforms\n",
    "    transforms = hms_1D_augmentations(augment_args=augment_args)\n",
    "    return transforms\n",
    "\n",
    "\n",
    "if DEBUG:\n",
    "    for i in range(len(configs)):\n",
    "        transforms = get_transforms(configs[i][0])\n",
    "        print(\"audio_val\\n\", transforms[\"audio_val\"])\n",
    "        print(\"torch_val\\n\", transforms[\"torch_val\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spectrogram_from_eegの処理がdataloader内で行うと遅いので、別で行う。\n",
    "def create_eeg_spec(denoise_wavelet=None):\n",
    "    paths_eegs = list(GLOBAL_CFG.eeg_dir_path.iterdir())\n",
    "    print(f\"There are {len(paths_eegs)} EEG spectrograms\")\n",
    "    all_eegs = {}\n",
    "    counter = 0\n",
    "    save_dir = TMP / f\"EEG_Spectrograms\"\n",
    "    save_dir.mkdir(parents=True, exist_ok=True)\n",
    "    for file_path in tqdm(paths_eegs):\n",
    "        file_path = str(file_path)\n",
    "        eeg_id = file_path.split(\"/\")[-1].split(\".\")[0]\n",
    "        save_path = save_dir / f\"{eeg_id}.npy\"\n",
    "        eeg_spectrogram = spectrogram_from_eeg(file_path, denoise_wavelet=denoise_wavelet, display=counter < 1)\n",
    "        # all_eegs[int(eeg_id)] = eeg_spectrogram\n",
    "        np.save(save_path, eeg_spectrogram)\n",
    "        counter += 1\n",
    "    return save_path.parent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### model loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_weight(checkpoint, net):\n",
    "    ckpt = torch.load(checkpoint, map_location=f\"cuda:{GLOBAL_CFG.gpu}\")[\"state_dict\"]\n",
    "    ckpt = {k[k.find(\".\") + 1 :]: v for k, v in ckpt.items()}\n",
    "    missing_keys, unexpected_keys = net.load_state_dict(ckpt, strict=False)\n",
    "    print(f\"\\nload checkpoint: {checkpoint}\\n\")\n",
    "    if len(missing_keys) != 0 or len(unexpected_keys) != 0:\n",
    "        print(\"====================================\")\n",
    "        print(\"missing_keys:\", missing_keys)\n",
    "        print(\"unexpecte_keys:\", unexpected_keys)\n",
    "        print(\"====================================\")\n",
    "    return net\n",
    "\n",
    "\n",
    "def get_models_from_checkkpoints(ckpt_paths, configs):\n",
    "    model_list = []\n",
    "    for ckpt, config in zip(ckpt_paths, configs):\n",
    "        model_args = copy.deepcopy(config[\"model\"])\n",
    "        model_args.load_checkpoint = None\n",
    "        for key in model_args.args.keys():\n",
    "            if \"pretrained\" in key:\n",
    "                model_args.args[key] = False\n",
    "        model = getattr(MODELS, model_args.name)(**model_args.args)\n",
    "        model.to(GLOBAL_CFG.gpu)\n",
    "        model = load_weight(ckpt, model)\n",
    "        model.eval()\n",
    "        model_list.append(model)\n",
    "    return model_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### data loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataloader(data_module, config, transforms, batch_size=1, num_workers=4):\n",
    "    dataset_args = copy.deepcopy(config[\"dataset\"])\n",
    "    dataset_args.csv_path = str(GLOBAL_CFG.csv_path)\n",
    "    dataset_args.spec_dir_path = str(GLOBAL_CFG.spec_dir_path)\n",
    "    dataset_args.eeg_dir_path = str(GLOBAL_CFG.eeg_dir_path)\n",
    "    eeg_spec_dir_path = dataset_args.get(\"eeg_spec_dir_path\", None)\n",
    "    # TODO: modelごとに異なるEEG_specを使う場合は修正の必要あり\n",
    "    if eeg_spec_dir_path is not None and KERNEL:\n",
    "        dataset_args.eeg_spec_dir_path = str(TMP / \"EEG_Spectrograms\")\n",
    "\n",
    "    # 推論時は事前作成済みのデータは使わないのでオフにする(configから削除したのでコメントアウト)\n",
    "    # dataset_args.spec_npy_path = None\n",
    "\n",
    "    # pseudo_label用に学習した場合は、Falseにすることで、Testデータを推論するようにする。\n",
    "    if hasattr(dataset_args, \"for_pseudo_label\"):\n",
    "        dataset_args.for_pseudo_label = False\n",
    "    if KERNEL or (MODE == \"test\"):\n",
    "        dataset_args.fold = None\n",
    "    \n",
    "\n",
    "    # modeによらずtestモードのデータセットを作成。(csvファイルの全行を予測)\n",
    "    dataset = data_module(mode=\"test\", transforms=transforms, **dataset_args)\n",
    "\n",
    "    loader = DataLoader(\n",
    "        dataset=dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=False,\n",
    "    )\n",
    "    return loader\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tta_pred(net, input):\n",
    "    preds = []\n",
    "    if \"Inversion\" in GLOBAL_CFG.tta_type:\n",
    "        y = net(-1 * input)\n",
    "        preds.append(y[\"pred\"])\n",
    "    if \"Reverse\" in GLOBAL_CFG.tta_type:\n",
    "        y = net(torch.flip(input, dims=[1]))\n",
    "        preds.append(y[\"pred\"])\n",
    "    if \"ChannelSwap\" in GLOBAL_CFG.tta_type:\n",
    "        b, t, c = input.shape\n",
    "        mid = c // 2\n",
    "        input = torch.cat((input[..., mid:], input[..., :mid]), dim=-1)\n",
    "        y = net(input)\n",
    "        preds.append(y[\"pred\"])\n",
    "    preds = torch.stack(preds, dim=0).mean(dim=0)\n",
    "    return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_inference_loop(model_list, dataloader, transform=None):\n",
    "    \"\"\"test時のループ\n",
    "    各foldのモデルで同じデータセットを予測し平均を取り、バッチ方向に連結。\n",
    "    \"\"\"\n",
    "    pred_list = []\n",
    "    logit_list = []\n",
    "    with torch.no_grad():\n",
    "        for i, batch in enumerate(tqdm(dataloader)):\n",
    "            eeg = batch[\"eeg\"].to(GLOBAL_CFG.gpu)\n",
    "            kspec = batch.get(\"Kspec\")\n",
    "            if (transform is not None) and (kspec is not None):\n",
    "                kspec = transform(kspec.to(GLOBAL_CFG.gpu))\n",
    "\n",
    "            preds = []\n",
    "            logits = []\n",
    "            for net in model_list:\n",
    "                if len(GLOBAL_CFG.tta_type) > 0:\n",
    "                    pred = tta_pred(net, eeg)\n",
    "                else:\n",
    "                    y = net(eeg)\n",
    "                    pred = y[\"pred\"]\n",
    "                logits.append(pred) # n, b, c\n",
    "                pred = F.softmax(pred, dim=1)\n",
    "                preds.append(pred) # n, b, c\n",
    "            preds = torch.stack(preds, dim=0).mean(0) # b, c\n",
    "            logits = torch.stack(logits, dim=0).mean(0) # b, c\n",
    "            preds = preds.detach().cpu().numpy()\n",
    "            logits = logits.detach().cpu().numpy()\n",
    "            pred_list.append(preds)\n",
    "            logit_list.append(logits)\n",
    "            if DEBUG and i >= DEBUG_SAMPLE_NUM:\n",
    "                break        \n",
    "\n",
    "    pred_arr = np.concatenate(pred_list)\n",
    "    logit_arr = np.concatenate(logit_list)\n",
    "    del pred_list, logit_list\n",
    "    return {\"pred_arr\": pred_arr, \"logit_arr\": logit_arr}\n",
    "\n",
    "def run_validatioan_loop(model_list, dataloader_list, transform=None):\n",
    "    \"\"\"validation時のループ\n",
    "    各foldごとに別のデータセットを予測しすべてをバッチ方向に連結。\n",
    "    \"\"\"\n",
    "    pred_list = []\n",
    "    logit_list = []\n",
    "    with torch.no_grad():\n",
    "        for net, dataloader in zip(model_list, dataloader_list):\n",
    "            for i, batch in enumerate(tqdm(dataloader)):\n",
    "                eeg = batch[\"eeg\"].to(GLOBAL_CFG.gpu)\n",
    "                kspec = batch.get(\"Kspec\")\n",
    "                if (transform is not None) and (kspec is not None):\n",
    "                    kspec = transform(kspec.to(GLOBAL_CFG.gpu))\n",
    "                if len(GLOBAL_CFG.tta_type) > 0:\n",
    "                    pred = tta_pred(net, eeg)\n",
    "                else:\n",
    "                    y = net(eeg)\n",
    "                    pred = y[\"pred\"]\n",
    "                logit = pred\n",
    "                pred = F.softmax(pred, dim=1)\n",
    "                logit = logit.detach().cpu().numpy()\n",
    "                pred = pred.detach().cpu().numpy()\n",
    "                logit_list.append(logit)\n",
    "                pred_list.append(pred)\n",
    "                if DEBUG and i >= DEBUG_SAMPLE_NUM:\n",
    "                    break        \n",
    "\n",
    "    pred_arr = np.concatenate(pred_list)\n",
    "    logit_arr = np.concatenate(logit_list)\n",
    "    del pred_list, logit_list\n",
    "    return {\"pred_arr\": pred_arr, \"logit_arr\": logit_arr} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/torch/functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at /usr/local/src/pytorch/aten/src/ATen/native/TensorShape.cpp:3483.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "load checkpoint: ../checkpoints/run-20240405_152214-wm5w0ew3-1D_cls_RTpIcS_LS_Wavenet-maxxvitv2n_1e-3_standard_e50_warmup_downwample_foldV2_finetune_fold_0/epoch=28-val_metric_kldiv_high_votes=0.240.ckpt\n",
      "\n",
      "\n",
      "load checkpoint: ../checkpoints/run-20240405_152227-th1rruwk-1D_cls_RTpIcS_LS_Wavenet-maxxvitv2n_1e-3_standard_e50_warmup_downwample_foldV2_finetune_fold_1/epoch=20-val_metric_kldiv_high_votes=0.218.ckpt\n",
      "\n",
      "\n",
      "load checkpoint: ../checkpoints/run-20240405_152228-xhwtkxba-1D_cls_RTpIcS_LS_Wavenet-maxxvitv2n_1e-3_standard_e50_warmup_downwample_foldV2_finetune_fold_2/epoch=25-val_metric_kldiv_high_votes=0.257.ckpt\n",
      "\n",
      "\n",
      "load checkpoint: ../checkpoints/run-20240405_165938-x123rriq-1D_cls_RTpIcS_LS_Wavenet-maxxvitv2n_1e-3_standard_e50_warmup_downwample_foldV2_finetune_fold_3/epoch=21-val_metric_kldiv_high_votes=0.241.ckpt\n",
      "\n",
      "\n",
      "load checkpoint: ../checkpoints/run-20240405_170053-kadak6et-1D_cls_RTpIcS_LS_Wavenet-maxxvits_1e-3_standard_e50_warmup_downwample_foldV2_finetune_fold_0/epoch=27-val_metric_kldiv_high_votes=0.240.ckpt\n",
      "\n",
      "\n",
      "load checkpoint: ../checkpoints/run-20240405_165949-lbjfygkz-1D_cls_RTpIcS_LS_Wavenet-maxxvits_1e-3_standard_e50_warmup_downwample_foldV2_finetune_fold_1/epoch=27-val_metric_kldiv_high_votes=0.216.ckpt\n",
      "\n",
      "\n",
      "load checkpoint: ../checkpoints/run-20240405_183700-wgsxjdxt-1D_cls_RTpIcS_LS_Wavenet-maxxvits_1e-3_standard_e50_warmup_downwample_foldV2_finetune_fold_2/epoch=27-val_metric_kldiv_high_votes=0.255.ckpt\n",
      "\n",
      "\n",
      "load checkpoint: ../checkpoints/run-20240405_184943-9u133n8x-1D_cls_RTpIcS_LS_Wavenet-maxxvits_1e-3_standard_e50_warmup_downwample_foldV2_finetune_fold_3/epoch=06-val_metric_kldiv_high_votes=0.250.ckpt\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/timm/models/_factory.py:114: UserWarning: Mapping deprecated model name tf_efficientnet_b4_ns to current tf_efficientnet_b4.ns_jft_in1k.\n",
      "  model = create_fn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "load checkpoint: ../checkpoints/run-20240405_185147-8gjfntzq-1D_cls_RTpIcS_LS_Wavenet-effnetb4_1e-3_standard_e15_warmup_downwample_foldV2_finetune_fold_0/epoch=06-val_metric_kldiv_high_votes=0.272.ckpt\n",
      "\n",
      "\n",
      "load checkpoint: ../checkpoints/run-20240405_202552-4q5x7wi1-1D_cls_RTpIcS_LS_Wavenet-effnetb4_1e-3_standard_e15_warmup_downwample_foldV2_finetune_fold_1/epoch=08-val_metric_kldiv_high_votes=0.240.ckpt\n",
      "\n",
      "\n",
      "load checkpoint: ../checkpoints/run-20240405_203742-j6v7ia0d-1D_cls_RTpIcS_LS_Wavenet-effnetb4_1e-3_standard_e15_warmup_downwample_foldV2_finetune_fold_2/epoch=09-val_metric_kldiv_high_votes=0.276.ckpt\n",
      "\n",
      "\n",
      "load checkpoint: ../checkpoints/run-20240405_195044-wteo0zct-1D_cls_RTpIcS_LS_Wavenet-effnetb4_1e-3_standard_e15_warmup_downwample_foldV2_finetune_fold_3/epoch=04-val_metric_kldiv_high_votes=0.273.ckpt\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d59b4985d83d4ddbabc7f846379ab225",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1623 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ae3e66236c144c09d5a757c2089b9ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1562 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb93a7ddbb424e12b5b675d25c975f1b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2046 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db247dee6e56497d9e4e106104c5b865",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1445 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d5c57cf977c74f698fe002213969059d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1623 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ae29a2c0ec946b9bcc78aa365091e6d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1562 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "87537d9f0dcd45638944d12ca35f0014",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2046 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "817c4e6a29bf434db5debff95b13c7a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1445 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "02f7b667fbb44105acc2d7327f372ee7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1623 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dbaa5337a38d4c2080427dd72ad5409d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1562 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "92a81a252eb242d49fe1d81933a42e1a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2046 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2da7a709021b43bd8776ea1759fa40c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1445 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "369"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ckpts_list, configs_list = load_configs(GLOBAL_CFG.checkpoints)\n",
    "models_list = []\n",
    "for ckpts, configs in zip(ckpts_list, configs_list):\n",
    "    models_list.append(get_models_from_checkkpoints(ckpts, configs))\n",
    "\n",
    "# TODO: モデルごとに必要なeeg_specを作れるようにする。(現状は一つのみ)\n",
    "# kernelの場合事前にeeg_specを作っておく。\n",
    "if KERNEL and hasattr(configs_list[0][0].dataset, \"eeg_spec_dir_path\"):\n",
    "    suffix = configs_list[0][0].dataset.eeg_spec_dir_path.split(\"_\")[-1]\n",
    "    # パスの最後にデノイズ方法が書いてあればそれを使う。書いてない場合は使わない\n",
    "    denoise_wavelet = suffix if suffix in pywt.wavelist() else None\n",
    "    create_eeg_spec(denoise_wavelet=denoise_wavelet)\n",
    "\n",
    "pred = []\n",
    "logit = []\n",
    "for ckpts, configs, models, data_module in zip(ckpts_list, configs_list, models_list, GLOBAL_CFG.datasets):\n",
    "    # transformはmodel(5fold)ごとに一つ作成\n",
    "    transforms = get_transforms(configs[0])\n",
    "    if KERNEL or (MODE == \"test\"): # kernel or テスト時は一つのdataloaderで良い。\n",
    "        dataloader = get_dataloader(\n",
    "            data_module,\n",
    "            configs[0],\n",
    "            transforms[\"audio_val\"],\n",
    "            batch_size=GLOBAL_CFG.batch_size,\n",
    "            num_workers=GLOBAL_CFG.n_workers,\n",
    "        )\n",
    "        result = run_inference_loop(\n",
    "            models, dataloader, transform=transforms[\"torch_val\"]\n",
    "        )\n",
    "        pred_arr = result[\"pred_arr\"]\n",
    "        logit_arr = result[\"logit_arr\"]\n",
    "    else: # validatiaon時はdataloaderをそれぞれ作る。\n",
    "        dataloaders = []\n",
    "        for config, model in zip(configs, models):\n",
    "            dataloader = get_dataloader(\n",
    "                data_module,\n",
    "                config,\n",
    "                transforms[\"audio_val\"],\n",
    "                batch_size=GLOBAL_CFG.batch_size,\n",
    "                num_workers=GLOBAL_CFG.n_workers,\n",
    "            )\n",
    "            dataloaders.append(dataloader)\n",
    "        result = run_validatioan_loop(\n",
    "            models, dataloaders, transform=transforms[\"torch_val\"]\n",
    "        )\n",
    "        pred_arr = result[\"pred_arr\"]\n",
    "        logit_arr = result[\"logit_arr\"]\n",
    "        # validation時はdfの情報を持っておく\n",
    "        dfs = []\n",
    "        for dataloader in dataloaders:\n",
    "            if DEBUG:\n",
    "                dfs.append(dataloader.dataset.df.iloc[:(DEBUG_SAMPLE_NUM+1)*GLOBAL_CFG.batch_size])\n",
    "            else:\n",
    "                dfs.append(dataloader.dataset.df)\n",
    "        train_df = pd.concat(dfs).reset_index(drop=True)\n",
    "\n",
    "    pred.append(pred_arr)\n",
    "    logit.append(logit_arr)\n",
    "\n",
    "pred_average = np.average(pred, axis=0, weights=GLOBAL_CFG.model_weight)\n",
    "\n",
    "del models_list\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def postprocess(pred, scale_coef=0.05):\n",
    "    \"\"\"ソフトマックス関数の出力を0.5に近づける調整\"\"\"\n",
    "    # 0.5からの距離に基づいて調整\n",
    "    adjusted = pred + (0.5 - pred) * scale_coef  # 調整係数\n",
    "    # 正規化して総和を1に保つ\n",
    "    adjusted_normalized = adjusted / adjusted.sum(axis=1, keepdims=True)\n",
    "    return adjusted_normalized\n",
    "\n",
    "if GLOBAL_CFG.scale_coeff > 0:\n",
    "    pred_average = postprocess(pred_average, scale_coef=GLOBAL_CFG.scale_coeff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV_kldiv             :  0.7374159378339975\n",
      "CV_kldiv_high_votes  :  0.2660511044486631\n",
      "CV_kldiv_low_votes   :  1.019061608016942\n"
     ]
    }
   ],
   "source": [
    "# test.csvにはeeg_idとspectrogram_id, patient_idが格納されている。\n",
    "# sample_submissionにはeeg_id列とターゲットの列しか無いので、testに予測結果をマージしてからsample_subにマージ\n",
    "\n",
    "def calc_metric(pred_df, columns):\n",
    "    solution = pred_df.loc[:, [\"eeg_id\"] + CLASSES]\n",
    "    submission = pred_df.loc[:, [\"eeg_id\"] + columns].rename(\n",
    "        columns={c: C for c, C in zip(columns, CLASSES)}\n",
    "    )\n",
    "    cv = calc_kl_div(\n",
    "        solution=solution, submission=submission, row_id_column_name=\"eeg_id\"\n",
    "    )\n",
    "    return cv\n",
    "\n",
    "def save_sub(output_path, pred, df, sample_submission):\n",
    "    pred_df = pd.DataFrame(pred, columns=CLASSES)\n",
    "    pred_df = pd.concat([df[[\"eeg_id\"]], pred_df], axis=1)\n",
    "    if not DEBUG:#sample_submissionの順番に合わせる処理\n",
    "        pred_df = pd.merge(sample_submission[[\"eeg_id\"]], pred_df, on=\"eeg_id\", how=\"left\")\n",
    "    pred_df.to_csv(output_path, index=False)\n",
    "    # pred_df.head()\n",
    "    \n",
    "\n",
    "if KERNEL or MODE == \"test\":\n",
    "    df = pd.read_csv(GLOBAL_CFG.csv_path)\n",
    "    smpl_sub = pd.read_csv(DATA / \"sample_submission.csv\")\n",
    "\n",
    "    # submissionの保存\n",
    "    save_sub(OUTPUT/\"submission.csv\", pred_average, df, smpl_sub)\n",
    "\n",
    "    # ensemble用に各モデルのprobとlogitを保存\n",
    "    if OUTPUT_FOR_ENSEMBLE:\n",
    "        for i, (p, l) in enumerate(zip(pred, logit)):\n",
    "            save_sub(OUTPUT / f\"{GLOBAL_CFG.model_name[i]}_prob.csv\", p, df, smpl_sub)\n",
    "            save_sub(OUTPUT / f\"{GLOBAL_CFG.model_name[i]}_logit.csv\", l, df, smpl_sub)\n",
    "\n",
    "    # pred_df = pd.DataFrame(pred_average, columns=CLASSES)\n",
    "    # pred_df = pd.concat([df[[\"eeg_id\"]], pred_df], axis=1)\n",
    "\n",
    "    # if not DEBUG: #sample_submissionの順に合わせる処理\n",
    "    #     pred_df = pd.merge(\n",
    "    #         smpl_sub[[\"eeg_id\"]], pred_df, on=\"eeg_id\", how=\"left\"\n",
    "    #     )\n",
    "\n",
    "    # pred_df.to_csv(OUTPUT / \"submission.csv\", index=False)\n",
    "    # pred_df.head()\n",
    "\n",
    "else:\n",
    "    columns = [\"pred_\" + c for c in CLASSES]\n",
    "    pred_df = pd.DataFrame(pred_average, columns=columns)\n",
    "    pred_df = pd.concat([train_df, pred_df], axis=1)\n",
    "    cv = calc_metric(pred_df, columns)\n",
    "    high_vote_cv = calc_metric(pred_df[pred_df[\"n_votes\"] >= 10], columns)\n",
    "    low_vote_cv = calc_metric(pred_df[pred_df[\"n_votes\"] < 10], columns)\n",
    "    print(\"CV_kldiv             : \", cv)\n",
    "    print(\"CV_kldiv_high_votes  : \", high_vote_cv)\n",
    "    print(\"CV_kldiv_low_votes   : \", low_vote_cv)\n",
    "    \n",
    "    if MODE == \"val\" and not DEBUG:\n",
    "        pred_df.to_csv(OUTPUT / \"prediction.csv\", index=False)\n",
    "        pred_df.head()\n",
    "\n",
    "    # モデルごとの確率値とlogitを保存\n",
    "    for i, (p, l) in enumerate(zip(pred, logit)):\n",
    "        cols_p = [\"prob_\" + c for c in CLASSES]\n",
    "        cols_l = [\"logit_\" + c for c in CLASSES]\n",
    "        sub_prob_df = pd.DataFrame(p, columns=cols_p)\n",
    "        sub_logit_df = pd.DataFrame(l, columns=cols_l)\n",
    "        sub_prob_df = pd.concat([train_df, sub_prob_df], axis=1)\n",
    "        sub_logit_df = pd.concat([train_df, sub_logit_df], axis=1)\n",
    "        if MODE == \"val\" and not DEBUG:\n",
    "            sub_prob_df.to_csv(OUTPUT / f\"prob_{GLOBAL_CFG.model_name[i]}_oof.csv\", index=False)\n",
    "            sub_logit_df.to_csv(OUTPUT / f\"logit_{GLOBAL_CFG.model_name[i]}_oof.csv\", index=False)\n",
    "            pred_df.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display(pred_df.head())\n",
    "# display(sub_prob_df.head())\n",
    "# display(sub_logit_df.head())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
